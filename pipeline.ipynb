{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b9e17033d4585bb621afd49f3579f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "pipe.safety_checker = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to generate images and save them to a folder\n",
    "\n",
    "def gen_images(prompt, folder_name):\n",
    "    for i in range(100):\n",
    "        image = pipe(prompt).images[0]\n",
    "        image.save(f\"{folder_name}/image_{i+1}.png\")\n",
    "    print(\"Done generating images\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embeddings(images):\n",
    "  \"\"\"\n",
    "    This function takes a list of images as input,\n",
    "    returning a list of clip embeddings of the images.\n",
    "\n",
    "    :param images: The input image as a string.\n",
    "    :return: A list of clip embeddings.\n",
    "    \"\"\"\n",
    "  inputs = clip_processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "  return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_from_folder(folder_name):\n",
    "    embeddings = []\n",
    "    for img in os.listdir(folder_name):\n",
    "        image = Image.open(f\"{folder_name}/{img}\")\n",
    "        image = image.convert(\"RGB\")\n",
    "        embeddings.append(get_clip_embeddings([image]))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding of dog images folder\n",
    "dog_folder = \"dog images\"\n",
    "dog_embeddings = get_embedding_from_folder(dog_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dog_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to numpy array\n",
    "dog_embeddings = np.array([embedding.cpu().numpy() for embedding in dog_embeddings]).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clusters = KMeans(n_clusters=10, init='k-means++').fit(dog_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get different images from each cluster\n",
    "def get_images_from_cluster(cluster_number, folder_name):\n",
    "    cluster_images = []\n",
    "    for i, label in enumerate(kmeans_clusters.labels_):\n",
    "        if label == cluster_number:\n",
    "            cluster_images.append(f\"{folder_name}/image_{i+1}.png\")\n",
    "    return cluster_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images for all the clusters\n",
    "cluster_images = []\n",
    "for i in range(10):\n",
    "    cluster_images.append(get_images_from_cluster(i, dog_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = kmeans_clusters.transform(dog_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_images = []\n",
    "\n",
    "for cluster_number in range(10):\n",
    "    cluster_indices = np.where(kmeans_clusters.labels_ == cluster_number)[0]\n",
    "    \n",
    "    cluster_distances = distance[cluster_indices, cluster_number]\n",
    "    \n",
    "    min_distance_indices = cluster_indices[np.argsort(cluster_distances)[:2]]\n",
    "    \n",
    "    closest_image_path = [f\"{dog_folder}/image_{idx+1}.png\" for idx in min_distance_indices]\n",
    "    closest_images.append(closest_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog images/image_53.png', 'dog images/image_9.png'],\n",
       " ['dog images/image_50.png', 'dog images/image_60.png'],\n",
       " ['dog images/image_58.png', 'dog images/image_41.png'],\n",
       " ['dog images/image_35.png', 'dog images/image_40.png'],\n",
       " ['dog images/image_72.png', 'dog images/image_44.png'],\n",
       " ['dog images/image_79.png', 'dog images/image_96.png'],\n",
       " ['dog images/image_59.png', 'dog images/image_86.png'],\n",
       " ['dog images/image_32.png'],\n",
       " ['dog images/image_12.png', 'dog images/image_30.png'],\n",
       " ['dog images/image_63.png', 'dog images/image_82.png']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
